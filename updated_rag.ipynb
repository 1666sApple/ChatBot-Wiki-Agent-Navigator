{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import cassio\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "ASTRA_DB_TOKEN = os.getenv('ASTRA_DB_TOKEN')\n",
        "ASTRA_DB_ID = os.getenv('ASTRA_DB_ID')\n",
        "\n",
        "# Initialize cassio database session (assuming session and keyspace)\n",
        "cassio.init(token=ASTRA_DB_TOKEN, database_id=ASTRA_DB_ID)\n",
        "session = cassio.get_session()\n",
        "keyspace = \"your_keyspace\"  # Ensure you set the correct keyspace\n",
        "\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "load_dotenv()\n",
        "HF_API_KEY = os.getenv('HF_API_KEY')\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embeddings,\n",
        "    table_name=\"qa_table\",\n",
        "    session=session,\n",
        "    keyspace=keyspace\n",
        ")\n",
        "\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "\n",
        "astra_vector_store.add_documents(doc_splits)\n",
        "print(f\"Inserted {len(doc_splits)} documents\")\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)\n",
        "\n",
        "retriever = astra_vector_store.as_retriever()\n",
        "\n",
        "from typing import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "load_dotenv()\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "class RouteQuery(BaseModel):\n",
        "    datasource: Literal['vectorstore', 'wikisearch'] = Field(\n",
        "        ...,\n",
        "        description=\"Given the user query, choose to route it to wikipedia or vectorstore.\"\n",
        "    )\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY, \n",
        "    model_name=\"Llama-3.1-70B-Versatile\"\n",
        ")\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "system = \"\"\"\n",
        "You are an expert at routing a user question to a vectorstore or wikipedia.\n",
        "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
        "Use the vectorstore for questions on these topics, otherwise perform a wikipedia search.\n",
        "\"\"\"\n",
        "\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', system),\n",
        "        ('human', \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ques_router = route_prompt | structured_llm_router\n",
        "\n",
        "\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "\n",
        "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=256)\n",
        "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]\n",
        "\n",
        "def retrieve(state):\n",
        "    print(f\"Retrieving from vectorstore:\")\n",
        "    question = state['question']\n",
        "    documents = retriever.retrieve(question)\n",
        "    return {\n",
        "        \"documents\": [doc.page_content for doc in documents],\n",
        "        \"questions\": question\n",
        "    }\n",
        "\n",
        "def wiki_search(state):\n",
        "    print(f\"Performing Wiki Search:\")\n",
        "    question = state['question']\n",
        "    results = wiki.invoke(\n",
        "        {\n",
        "            \"query\": question\n",
        "        }\n",
        "    )\n",
        "    return {\n",
        "        \"documents\": [results],\n",
        "        \"questions\": question\n",
        "    }\n",
        "\n",
        "def route_question(state):\n",
        "    print(f\"Routing question\")\n",
        "    question = state[\"question\"]\n",
        "    source = ques_router.invoke(\n",
        "        {\n",
        "            \"question\": question\n",
        "        }\n",
        "    )\n",
        "    if source.datasource == \"wikisearch\":\n",
        "        print(f\"Routing question to Wiki Search\")\n",
        "        return \"wiki_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(f\"Routing question to Vectorstore (RAG SYSTEM)\")\n",
        "        return \"retrieve\"\n",
        "\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"wiki_search\", wiki_search)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"wiki_search\": \"wiki_search\",\n",
        "        \"retrieve\": \"retrieve\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"retrieve\", END)\n",
        "workflow.add_edge(\"wiki_search\", END)\n",
        "\n",
        "app = workflow.compile()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}